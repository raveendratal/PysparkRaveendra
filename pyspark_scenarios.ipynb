{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3330d462-229a-45a1-a50f-b18982dee7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# PySpark Scenarios with Examples\n",
    "\n",
    "This notebook demonstrates common PySpark data analysis scenarios with practical examples. Each scenario showcases a specific use case, including sales aggregation, identifying top-selling products, analyzing customer purchase patterns, and calculating customer lifetime value.\n",
    "\n",
    "**Prepared by TR Raveendra**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f15ccd80-510d-4720-8b1c-c651c776d82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f738c59-1dbc-4203-961d-13b5071b4c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 1: Calculating Sales Totals\n",
    "This scenario calculates the total price and total quantity sold for each product using \n",
    "\n",
    "groupBy() and agg() functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796e3a9e-cc4d-42f8-9ff8-d82cb7106c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------+\n|product_name|total_price|total_quantity|\n+------------+-----------+--------------+\n|Laptop      |2400       |3             |\n|Smartphone  |800        |1             |\n+------------+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample sales data\n",
    "sales_data = [\n",
    "    {\"product_id\": 1, \"product_name\": \"Laptop\", \"price\": 1200, \"quantity\": 2},\n",
    "    {\"product_id\": 2, \"product_name\": \"Smartphone\", \"price\": 800, \"quantity\": 1},\n",
    "    {\"product_id\": 1, \"product_name\": \"Laptop\", \"price\": 1200, \"quantity\": 1},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# 3. Calculate sales totals using groupBy() and agg()\n",
    "sales_totals = df.groupBy(\"product_name\").agg(\n",
    "    sum(\"price\").alias(\"total_price\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity\")\n",
    ")\n",
    "\n",
    "# 4. Show the result\n",
    "sales_totals.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04b2b57c-a95d-4c06-8212-0997fb65405d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 2: Identifying Top-Selling Products\n",
    "This code identifies the top-selling products by quantity by using \n",
    "\n",
    "groupBy(), agg(), sort(), and limit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc35210-c78c-4944-821d-aeb3901703bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n|product_name|total_quantity|\n+------------+--------------+\n|      Laptop|             3|\n|  Smartphone|             1|\n+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample sales data\n",
    "sales_data = [\n",
    "    {\"product_id\": 1, \"product_name\": \"Laptop\", \"price\": 1200, \"quantity\": 2},\n",
    "    {\"product_id\": 2, \"product_name\": \"Smartphone\", \"price\": 800, \"quantity\": 1},\n",
    "    {\"product_id\": 1, \"product_name\": \"Laptop\", \"price\": 1200, \"quantity\": 1},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# 3. Aggregate, sort, and limit to find top-selling products\n",
    "top_selling_products = df.groupBy(\"product_name\").agg(\n",
    "    sum(\"quantity\").alias(\"total_quantity\")\n",
    ").sort(\n",
    "    \"total_quantity\", ascending=False\n",
    ").limit(3)\n",
    "\n",
    "# 4. Show the result\n",
    "top_selling_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b44a1be-c5cd-47a8-9028-f4cba5a51b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 3: Analyzing Customer Purchase Patterns\n",
    "### This script joins customer and sales data to analyze purchase patterns using \n",
    "\n",
    "join() and groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830a2a1a-eedb-4c01-821b-32469903d366",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n|     name|count(order_id)|\n+---------+---------------+\n|Raveendra|              1|\n|Reshwanth|              1|\n+---------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "customer_data = [\n",
    "    {\"customer_id\": 1, \"name\": \"Raveendra\", \"email\": \"Raveendra@gmail.com\"},\n",
    "    {\"customer_id\": 2, \"name\": \"Reshwanth\", \"email\": \"Reshwanth@gmail.com\"},\n",
    "]\n",
    "\n",
    "sales_data = [\n",
    "    {\"order_id\": 1, \"customer_id\": 1, \"product_id\": 1, \"quantity\": 1},\n",
    "    {\"order_id\": 2, \"customer_id\": 2, \"product_id\": 2, \"quantity\": 2},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrames\n",
    "customer_df = spark.createDataFrame(customer_data)\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# 3. Join and group to analyze purchase patterns\n",
    "joined_df = customer_df.join(sales_df, on=\"customer_id\")\n",
    "purchase_patterns = joined_df.groupBy(\"name\").agg(count(\"order_id\"))\n",
    "\n",
    "# 4. Show the result\n",
    "purchase_patterns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4ef54f1-84af-4b30-8d53-5eb856005de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 4: Calculating Customer Lifetime Value (CLV)\n",
    "This scenario calculates the total amount spent and total number of orders for each customer to determine their lifetime value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0249d35f-1e07-40e3-a463-6fa5ec89439d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+\n|customer_id|total_spent|total_orders|\n+-----------+-----------+------------+\n|          1|        150|           2|\n|          2|        200|           1|\n+-----------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "order_data = [\n",
    "    {\"order_id\": 1, \"customer_id\": 1, \"order_amount\": 100},\n",
    "    {\"order_id\": 2, \"customer_id\": 1, \"order_amount\": 50},\n",
    "    {\"order_id\": 3, \"customer_id\": 2, \"order_amount\": 200},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(order_data)\n",
    "\n",
    "# 3. Group by customer to calculate CLV\n",
    "clv_df = df.groupBy(\"customer_id\").agg(\n",
    "    sum(\"order_amount\").alias(\"total_spent\"),\n",
    "    count(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "\n",
    "# 4. Show the result\n",
    "clv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5cb5a34-258e-4a2c-9c6d-08ef86d16c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 5: Identifying Repeat Customers\n",
    "This code identifies customers who have placed more than one order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc5b4ba-a400-43f6-905f-f777651bc462",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|customer_id|count|\n+-----------+-----+\n|          1|    2|\n|          2|    1|\n|          3|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "order_data = [\n",
    "    {\"order_id\": 1, \"customer_id\": 1},\n",
    "    {\"order_id\": 2, \"customer_id\": 2},\n",
    "    {\"order_id\": 3, \"customer_id\": 1},\n",
    "    {\"order_id\": 4, \"customer_id\": 3},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(order_data)\n",
    "\n",
    "# 3. Group by customer_id and count orders\n",
    "repeat_customer_df = df.groupBy(\"customer_id\").count()\n",
    "\n",
    "# 4. Show the result\n",
    "repeat_customer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "995234d9-7dd1-44a1-bb0a-2896c6e5d24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 6: Analyzing Order Placement Frequency\n",
    "This script analyzes order frequency by year, demonstrating the use of the \n",
    "\n",
    "year() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b307b65-a1e2-4b5b-b55e-d9cf20e8a046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----+\n|customer_id|order_year|count|\n+-----------+----------+-----+\n|          1|      2023|    2|\n|          2|      2023|    1|\n+-----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "order_data = [\n",
    "    {\"order_id\": 1, \"customer_id\": 1, \"order_date\": \"2023-10-01\"},\n",
    "    {\"order_id\": 2, \"customer_id\": 1, \"order_date\": \"2023-10-05\"},\n",
    "    {\"order_id\": 3, \"customer_id\": 2, \"order_date\": \"2023-10-10\"},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(order_data)\n",
    "\n",
    "# 3. Add a year column and then group to analyze frequency\n",
    "df = df.withColumn(\"order_year\", year(df[\"order_date\"]))\n",
    "order_frequency_df = df.groupBy(\"customer_id\", \"order_year\").count()\n",
    "\n",
    "# 4. Show the result\n",
    "order_frequency_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b65e14ea-b9b2-45c6-ad63-e94daf5ba089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 7: Calculating Salesman Commission\n",
    "This code calculates the commission for each salesman based on their total sales and commission rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b065979-f4ed-400a-a186-62a847be3fcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+-----------+----------+\n|commission_rate|sales|salesman_id|commission|\n+---------------+-----+-----------+----------+\n|            0.1| 1000|          1|     100.0|\n|           0.15| 2000|          2|     300.0|\n|            0.2| 3000|          3|     600.0|\n+---------------+-----+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "data = [\n",
    "    {\"salesman_id\": 1, \"sales\": 1000, \"commission_rate\": 0.1},\n",
    "    {\"salesman_id\": 2, \"sales\": 2000, \"commission_rate\": 0.15},\n",
    "    {\"salesman_id\": 3, \"sales\": 3000, \"commission_rate\": 0.2},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(data=data)\n",
    "\n",
    "# 3. Calculate commission using withColumn()\n",
    "df_with_commission = df.withColumn(\"commission\", col(\"sales\") * col(\"commission_rate\"))\n",
    "\n",
    "# 4. Show the result\n",
    "df_with_commission.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37040534-418a-4a75-8192-93db62ebf97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 8: Getting Unique Salesman IDs\n",
    "This scenario shows how to get a list of unique salesman IDs using the \n",
    "\n",
    "distinct() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acc5533-e57c-45ce-8510-546528dfdfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|salesman_id|\n+-----------+\n|          1|\n|          2|\n|          3|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "data = [\n",
    "    {\"salesman_id\": 1, \"sales\": 1000},\n",
    "    {\"salesman_id\": 1, \"sales\": 2000},\n",
    "    {\"salesman_id\": 2, \"sales\": 3000},\n",
    "    {\"salesman_id\": 2, \"sales\": 4000},\n",
    "    {\"salesman_id\": 3, \"sales\": 5000},\n",
    "]\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(data=data)\n",
    "\n",
    "# 3. Get unique salesman IDs using distinct()\n",
    "unique_salesman_ids = df.select(\"salesman_id\").distinct()\n",
    "\n",
    "# 4. Show the result\n",
    "unique_salesman_ids.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c63f16cd-dfdf-407f-b6ce-6915f3035579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Scenario 9: Filtering Salesmen by City\n",
    "This code filters the DataFrame to find all salesmen from a specific city (\n",
    "\n",
    "Bangalore) and then selects their name and city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78a1c88-814e-4673-af9c-e2d8576d98eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n|salesman_name|     city|\n+-------------+---------+\n|    Reshwanth|Bangalore|\n|    Raveendra|Bangalore|\n+-------------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data with a predefined schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "data = [\n",
    "    (1, \"Reshwanth\", \"Bangalore\"),\n",
    "    (2, \"Vikranth\", \"New York\"),\n",
    "    (3, \"Raveendra\", \"Bangalore\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"salesman_id\", IntegerType(), True),\n",
    "    StructField(\"salesman_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "])\n",
    "\n",
    "# 2. Create DataFrame\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# 3. Filter and select\n",
    "bangalore_salesmen = df.filter(col(\"city\") == \"Bangalore\")\n",
    "salesmen_info = bangalore_salesmen.select(\"salesman_name\", \"city\")\n",
    "\n",
    "# 4. Show the result\n",
    "salesmen_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f516d336-9a00-4f21-9945-8fadd6b68de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 10: Filtering Order Data\n",
    "This scenario filters a DataFrame to find orders from a specific delivery person and selects specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e807b0f-f40f-4ba8-895d-df9b6a1b3929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---+------+\n|order_number|      date|qty|amount|\n+------------+----------+---+------+\n|           2|2023-01-02|  5|    50|\n|           3|2023-01-03|  8|    80|\n+------------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data with a predefined schema\n",
    "data = [\n",
    "    (1, \"2023-01-01\", 10, 100, 100),\n",
    "    (2, \"2023-01-02\", 5, 50, 200),\n",
    "    (3, \"2023-01-03\", 8, 80, 200),\n",
    "    (4, \"2023-01-04\", 12, 120, 300),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_number\", IntegerType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"qty\", IntegerType()),\n",
    "    StructField(\"amount\", IntegerType()),\n",
    "    StructField(\"deliverypersonid\", IntegerType()),\n",
    "])\n",
    "\n",
    "# 2. Create DataFrame and cast date column\n",
    "df = spark.createDataFrame(data=data, schema=schema).withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# 3. Filter and select\n",
    "filtered_data = df.filter(col(\"deliverypersonid\") == 200)\n",
    "result = filtered_data.select(\"order_number\", \"date\", \"qty\", \"amount\")\n",
    "\n",
    "# 4. Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1421eba7-90ff-48c1-94e4-cd9b576e282d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 11: Getting First and Last Day of the Month\n",
    "This code demonstrates how to calculate the first and last day of the month for a given date column using trunc() and last_day()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1ab014-3d65-43ea-a246-d5d8227888ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------+\n|      date|first_day_month|last_day_month|\n+----------+---------------+--------------+\n|2023-01-15|     2023-01-01|    2023-01-31|\n|2023-02-20|     2023-02-01|    2023-02-28|\n|2023-03-05|     2023-03-01|    2023-03-31|\n|2023-02-10|     2023-02-01|    2023-02-28|\n+----------+---------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data with a date column\n",
    "data = [\n",
    "    (\"2023-01-15\",),\n",
    "    (\"2023-02-20\",),\n",
    "    (\"2023-03-05\",),\n",
    "    (\"2023-02-10\",),\n",
    "]\n",
    "\n",
    "columns = [\"date\"]\n",
    "\n",
    "# 2. Create DataFrame and convert string to DateType\n",
    "df = spark.createDataFrame(data, columns).withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "\n",
    "# 3. Calculate first and last dates of the month\n",
    "df_result = df.withColumn(\n",
    "    \"first_day_month\",\n",
    "    trunc(df[\"date\"], \"month\")\n",
    ").withColumn(\n",
    "    \"last_day_month\",\n",
    "    last_day(df[\"date\"])\n",
    ")\n",
    "\n",
    "# 4. Show the result\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffd145d-d99a-4e7e-a397-7ba78bc15f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 12: Finding First Visited Customers\n",
    "This scenario uses a window function to find the first visit for each customer, partitioned by customer ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb002bc-a237-409a-a8ee-ca6bc70bee13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+\n|customer_id|store_id|visit_date|\n+-----------+--------+----------+\n|      Cust1|  StoreA|2023-01-01|\n|      Cust2|  StoreB|2023-01-02|\n|      Cust3|  StoreB|2023-01-04|\n+-----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data\n",
    "data = [\n",
    "    (\"Cust1\", \"StoreA\", \"2023-01-01\"),\n",
    "    (\"Cust2\", \"StoreB\", \"2023-01-02\"),\n",
    "    (\"Cust1\", \"StoreA\", \"2023-01-03\"),\n",
    "    (\"Cust3\", \"StoreB\", \"2023-01-04\"),\n",
    "    (\"Cust2\", \"StoreA\", \"2023-01-05\"),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"store_id\", StringType(), True),\n",
    "    StructField(\"visit_date\", StringType(), True),\n",
    "])\n",
    "\n",
    "# 2. Create DataFrame and cast date column\n",
    "df = spark.createDataFrame(data, schema).withColumn(\"visit_date\", to_date(col(\"visit_date\")))\n",
    "\n",
    "# 3. Define a window specification\n",
    "windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"visit_date\")\n",
    "\n",
    "# 4. Find the first visit date for each customer\n",
    "df_result = df.withColumn(\n",
    "    \"first_time_visited\",\n",
    "    min(col(\"visit_date\")).over(windowSpec)\n",
    ").filter(\n",
    "    col(\"visit_date\") == col(\"first_time_visited\")\n",
    ")\n",
    "\n",
    "# 5. Select the required columns and show\n",
    "result = df_result.select(\"customer_id\", \"store_id\", \"visit_date\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e0056d-b1fa-492f-87dd-6b203c2b070f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 13: Union of Two DataFrames\n",
    "This scenario demonstrates how to combine two DataFrames with the same schema into a single DataFrame using the union() operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92817bed-0bbc-47fb-96ed-42d689efdc2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|product_name|price|\n+------------+-----+\n|      Laptop| 1200|\n|  Smartphone|  800|\n|      Tablet|  500|\n|  Smartwatch|  300|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create sample data for two DataFrames\n",
    "data1 = [(\"Laptop\", 1200), (\"Smartphone\", 800)]\n",
    "data2 = [(\"Tablet\", 500), (\"Smartwatch\", 300)]\n",
    "columns = [\"product_name\", \"price\"]\n",
    "\n",
    "# 2. Create the DataFrames\n",
    "df1 = spark.createDataFrame(data=data1, schema=columns)\n",
    "df2 = spark.createDataFrame(data=data2, schema=columns)\n",
    "\n",
    "# 3. Union the two DataFrames\n",
    "union_df = df1.union(df2)\n",
    "\n",
    "# 4. Show the result\n",
    "union_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9737b977-2bce-4753-88f8-8dd5766e6d1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 14: Exploding an Array Column\n",
    "This script shows how to transform a DataFrame with an array column by creating a new row for each element in the array using the explode() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1050c76b-54c8-48f1-b98c-7afb7ec5d734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------+\n|   product|              brands|  brand|\n+----------+--------------------+-------+\n|    Laptop|         [MSI, DELL]|    MSI|\n|    Laptop|         [MSI, DELL]|   DELL|\n|Smartphone|[Samsung, Apple, ...|Samsung|\n|Smartphone|[Samsung, Apple, ...|  Apple|\n|Smartphone|[Samsung, Apple, ...| Google|\n+----------+--------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DataFrame with an array column\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
    "\n",
    "data = [\n",
    "    (\"Laptop\", [\"MSI\", \"DELL\"]),\n",
    "    (\"Smartphone\", [\"Samsung\", \"Apple\", \"Google\"]),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"brands\", ArrayType(StringType()), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 2. Explode the 'brands' column\n",
    "exploded_df = df.withColumn(\"brand\", explode(df.brands))\n",
    "\n",
    "# 3. Show the result\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d64fe7a-7799-407a-8e20-d9ae9348b9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 15: Dropping Duplicate Rows\n",
    "This scenario demonstrates how to remove duplicate rows from a DataFrame based on all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00398328-4aae-4165-8fd4-46a139f87c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n|product_name|price|\n+------------+-----+\n|      Laptop| 1200|\n|  Smartphone|  800|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DataFrame with duplicate rows\n",
    "data = [\n",
    "    (\"Laptop\", 1200),\n",
    "    (\"Smartphone\", 800),\n",
    "    (\"Laptop\", 1200),\n",
    "]\n",
    "columns = [\"product_name\", \"price\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "# 2. Drop the duplicate rows\n",
    "distinct_df = df.dropDuplicates()\n",
    "\n",
    "# 3. Show the result\n",
    "distinct_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05c8467-67ac-4482-9aba-056f4d3bbf71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 16: Renaming a Column\n",
    "This code shows how to rename a column in a DataFrame from its original name to a new name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07b8960-7d89-4dd7-8a1f-5a84eb2aa301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|item_name|price|\n+---------+-----+\n|   Laptop| 1200|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DataFrame\n",
    "data = [(\"Laptop\", 1200)]\n",
    "columns = [\"product_name\", \"price\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "# 2. Rename the 'product_name' column to 'item_name'\n",
    "renamed_df = df.withColumnRenamed(\"product_name\", \"item_name\")\n",
    "\n",
    "# 3. Show the result\n",
    "renamed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da41657e-906f-4ce7-9ae4-b7e8a0ec1801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Scenario 17: Adding a New Column\n",
    "This script adds a new column to a DataFrame, in this case, a 'status' column with a fixed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee138d54-525a-454a-a7fc-90afc1bf3662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+------+\n|product_name|price|status|\n+------------+-----+------+\n|      Laptop| 1200|   New|\n|  Smartphone|  800|   New|\n+------------+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DataFrame\n",
    "data = [(\"Laptop\", 1200), (\"Smartphone\", 800)]\n",
    "columns = [\"product_name\", \"price\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "# 2. Add a new 'status' column\n",
    "df_with_status = df.withColumn(\"status\", lit(\"New\"))\n",
    "\n",
    "# 3. Show the result\n",
    "df_with_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfbb10c-6664-4a58-89d6-e71a82f01038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Detailed Documentation: PySpark Window Functions**\n",
    "\n",
    "**`unboundedPreceding`** and **`currentRow`** are keywords used in PySpark's window functions to define a **frame**, which is a set of rows within a window (or partition) on which a function operates. This frame moves as the function is applied to each row.\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Overview**\n",
    "\n",
    "In simple terms, a window function calculates a value for each row based on a set of rows related to the current row. This set of related rows is the **window frame**. The frame is defined by a `start` boundary and an `end` boundary.\n",
    "\n",
    "* **`unboundedPreceding`**: This is a frame boundary that represents the **first row** of the partition. No matter where the current row is, this boundary will always be at the very beginning of the partition. It is \"unbounded\" because it doesn't refer to a fixed number of rows; it just means \"start from the beginning.\"\n",
    "* **`currentRow`**: This is a frame boundary that represents the **current row** being processed. As the window function moves from row to row, the `currentRow` boundary changes to always be at the row currently under consideration.\n",
    "\n",
    "When you use `rowsBetween(Window.unboundedPreceding, Window.currentRow)`, you are telling Spark to define a frame for each row that starts at the first row of the partition and ends at the current row.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analogy and Example**\n",
    "\n",
    "Imagine you're tracking your daily steps. You want to know your total steps for the year up to the current day. Each day is a row in your data.\n",
    "\n",
    "* The **partition** is the entire year's worth of data.\n",
    "* The **window frame** for any given day is from January 1st (the `unboundedPreceding` row) up to that day (the `currentRow`).\n",
    "\n",
    "This combination is perfect for calculating metrics that are cumulative over time, such as **running totals**, **cumulative averages**, and **running counts**, because the calculation for each row includes all preceding rows in the same partition.\n",
    "\n",
    "Let's illustrate with a simple table and a running sum calculation:\n",
    "\n",
    "| store_id | product_id | sales_amount |\n",
    "| :--- | :--- | :--- |\n",
    "| Store A | ProductX | 100 |\n",
    "| Store A | ProductX | 150 |\n",
    "| Store A | ProductX | 50 |\n",
    "\n",
    "If you were to calculate a running sum on this data ordered by an implicit date, the window frame would change for each row:\n",
    "\n",
    "1.  **For the first row (`sales_amount` = 100):**\n",
    "    * The frame starts at `unboundedPreceding` (the first row).\n",
    "    * The frame ends at `currentRow` (the first row).\n",
    "    * The sum is **100**.\n",
    "\n",
    "2.  **For the second row (`sales_amount` = 150):**\n",
    "    * The frame starts at `unboundedPreceding` (the first row).\n",
    "    * The frame ends at `currentRow` (the second row).\n",
    "    * The sum is 100 + 150 = **250**.\n",
    "\n",
    "3.  **For the third row (`sales_amount` = 50):**\n",
    "    * The frame starts at `unboundedPreceding` (the first row).\n",
    "    * The frame ends at `currentRow` (the third row).\n",
    "    * The sum is 100 + 150 + 50 = **300**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106bae77-3891-4ed9-857d-be8f7395d137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------------+\n|store_id|product_id| sale_date|sales_amount|\n+--------+----------+----------+------------+\n|  StoreA|  ProductX|2023-01-01|         100|\n|  StoreA|  ProductX|2023-01-02|         150|\n|  StoreA|  ProductX|2023-01-03|          50|\n|  StoreA|  ProductY|2023-01-01|         200|\n|  StoreA|  ProductY|2023-01-02|         250|\n|  StoreB|  ProductX|2023-01-01|         300|\n|  StoreB|  ProductX|2023-01-02|         100|\n+--------+----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from pyspark.sql.functions import sum, avg, count, col, row_number, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    " \n",
    "# Create sample sales data\n",
    "data = [\n",
    "    (\"StoreA\", \"ProductX\", \"2023-01-01\", 100),\n",
    "    (\"StoreA\", \"ProductX\", \"2023-01-02\", 150),\n",
    "    (\"StoreA\", \"ProductX\", \"2023-01-03\", 50),\n",
    "    (\"StoreA\", \"ProductY\", \"2023-01-01\", 200),\n",
    "    (\"StoreA\", \"ProductY\", \"2023-01-02\", 250),\n",
    "    (\"StoreB\", \"ProductX\", \"2023-01-01\", 300),\n",
    "    (\"StoreB\", \"ProductX\", \"2023-01-02\", 100),\n",
    "]\n",
    "\n",
    "columns = [\"store_id\", \"product_id\", \"sale_date\", \"sales_amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns).withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c0174cf-b6c6-48ef-aca8-703a7c6b01da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 1: Running Total of Sales\n",
    "This code calculates the `cumulative sales amount` for each product within each store over time.\n",
    "\n",
    "`Partitioning`: The data is partitioned by store_id and product_id.\n",
    "\n",
    "`Ordering`: The rows within each partition are ordered by sale_date.\n",
    "\n",
    "`Frame`: The frame is defined as unboundedPreceding to currentRow, which includes all preceding rows up to the current row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb708fd4-4e57-46df-b810-b2c60e011ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8df7f5-1612-4ce5-965a-9a5540dca7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------------+-------------------+\n|store_id|product_id| sale_date|sales_amount|running_total_sales|\n+--------+----------+----------+------------+-------------------+\n|  StoreA|  ProductX|2023-01-01|         100|                100|\n|  StoreA|  ProductX|2023-01-02|         150|                250|\n|  StoreA|  ProductX|2023-01-03|          50|                300|\n|  StoreA|  ProductY|2023-01-01|         200|                200|\n|  StoreA|  ProductY|2023-01-02|         250|                450|\n|  StoreB|  ProductX|2023-01-01|         300|                300|\n|  StoreB|  ProductX|2023-01-02|         100|                400|\n+--------+----------+----------+------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define the window specification\n",
    "# 'rowsBetween(Window.unboundedPreceding, Window.currentRow)' sets the frame to include all rows from the start of the partition up to the current row.\n",
    "# This is commonly used for cumulative calculations (e.g., running totals).\n",
    "# It is NOT mandatory; if omitted, the default frame for ordered windows is 'rangeBetween(unboundedPreceding, currentRow)' for aggregation functions.\n",
    "window_spec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate the running total of sales\n",
    "df_with_running_total = df.withColumn(\n",
    "    \"running_total_sales\",\n",
    "    sum(\"sales_amount\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_with_running_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d43e05-e120-4c20-81af-38c8e4462094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+------------+------------------------+-----------------------------+\n|store_id|product_id|event_id|sales_amount|sum_sales_eventid_window|sum_sales_eventid_full_window|\n+--------+----------+--------+------------+------------------------+-----------------------------+\n|StoreA  |ProductX  |1000    |100         |250                     |300                          |\n|StoreA  |ProductX  |1500    |150         |300                     |300                          |\n|StoreA  |ProductX  |2000    |50          |200                     |300                          |\n|StoreA  |ProductY  |1200    |200         |200                     |450                          |\n|StoreA  |ProductY  |2200    |250         |250                     |450                          |\n|StoreB  |ProductX  |800     |300         |300                     |400                          |\n|StoreB  |ProductX  |2100    |100         |100                     |400                          |\n+--------+----------+--------+------------+------------------------+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.window import Window\n",
    " \n",
    "# ---------------------------\n",
    "# 1. Create sample dataset\n",
    "# ---------------------------\n",
    "data = [\n",
    "    (\"StoreA\", \"ProductX\", 1000, 100),\n",
    "    (\"StoreA\", \"ProductX\", 1500, 150),\n",
    "    (\"StoreA\", \"ProductX\", 2000, 50),\n",
    "    (\"StoreA\", \"ProductY\", 1200, 200),\n",
    "    (\"StoreA\", \"ProductY\", 2200, 250),\n",
    "    (\"StoreB\", \"ProductX\", 800, 300),\n",
    "    (\"StoreB\", \"ProductX\", 2100, 100),\n",
    "]\n",
    "columns = [\"store_id\", \"product_id\", \"event_id\", \"sales_amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define a window using rangeBetween\n",
    "#    - Partition by store_id + product_id\n",
    "#    - Order by event_id\n",
    "#    - For each row, look 500 before and 500 after in event_id\n",
    "# ---------------------------\n",
    "window_spec_range = (\n",
    "    Window.partitionBy(\"store_id\", \"product_id\")\n",
    "    .orderBy(col(\"event_id\"))\n",
    "    .rangeBetween(-500, 500)\n",
    ")\n",
    "\n",
    "df_range = df.withColumn(\n",
    "    \"sum_sales_eventid_window\",\n",
    "    sum(\"sales_amount\").over(window_spec_range)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Define a window over the FULL partition\n",
    "#    - Uses unboundedPreceding to unboundedFollowing\n",
    "#    - Includes ALL rows for the same store_id + product_id\n",
    "# ---------------------------\n",
    "window_spec_full = (\n",
    "    Window.partitionBy(\"store_id\", \"product_id\")\n",
    "    .orderBy(col(\"event_id\"))\n",
    "    .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    ")\n",
    "\n",
    "df_full = df_range.withColumn(\n",
    "    \"sum_sales_eventid_full_window\",\n",
    "    sum(\"sales_amount\").over(window_spec_full)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Show final result\n",
    "# ---------------------------\n",
    "df_full.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6df103b-51e2-43e7-9bc4-cd0b8d8f7f29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 2: Cumulative Average of Sales\n",
    "This example calculates the `running average of sales` for each product within a store. This is useful for tracking performance trends.\n",
    "\n",
    "`Partitioning`: By store_id and product_id.\n",
    "\n",
    "`Ordering`: By sale_date.\n",
    "\n",
    "`Frame`: The frame is unboundedPreceding to currentRow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206c5a47-7f6c-4c67-9173-aeb79e76f793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------------+-----------------+\n|store_id|product_id| sale_date|sales_amount|running_avg_sales|\n+--------+----------+----------+------------+-----------------+\n|  StoreA|  ProductX|2023-01-01|         100|            100.0|\n|  StoreA|  ProductX|2023-01-02|         150|            125.0|\n|  StoreA|  ProductX|2023-01-03|          50|            100.0|\n|  StoreA|  ProductY|2023-01-01|         200|            200.0|\n|  StoreA|  ProductY|2023-01-02|         250|            225.0|\n|  StoreB|  ProductX|2023-01-01|         300|            300.0|\n|  StoreB|  ProductX|2023-01-02|         100|            200.0|\n+--------+----------+----------+------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Calculate the running average of sales\n",
    "df_with_running_avg = df.withColumn(\n",
    "    \"running_avg_sales\",\n",
    "    avg(\"sales_amount\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_with_running_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bcc1635-7e67-4158-b056-d6897903f669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scenario 3: Counting Cumulative Transactions\n",
    "This code `counts the number of transactions` up to the current row for each product within each store.\n",
    "\n",
    "`Partitioning`: By store_id and product_id.\n",
    "\n",
    "`Ordering`: By sale_date.\n",
    "\n",
    "`Frame`: The frame is unboundedPreceding to currentRow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96733eb4-954a-4323-a2c5-75ef347223fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+------------+-----------------------+\n|store_id|product_id| sale_date|sales_amount|cumulative_transactions|\n+--------+----------+----------+------------+-----------------------+\n|  StoreA|  ProductX|2023-01-01|         100|                      1|\n|  StoreA|  ProductX|2023-01-02|         150|                      2|\n|  StoreA|  ProductX|2023-01-03|          50|                      3|\n|  StoreA|  ProductY|2023-01-01|         200|                      1|\n|  StoreA|  ProductY|2023-01-02|         250|                      2|\n|  StoreB|  ProductX|2023-01-01|         300|                      1|\n|  StoreB|  ProductX|2023-01-02|         100|                      2|\n+--------+----------+----------+------------+-----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"store_id\", \"product_id\").orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Count the cumulative number of transactions\n",
    "df_with_cumulative_count = df.withColumn(\n",
    "    \"cumulative_transactions\",\n",
    "    count(\"*\").over(window_spec)\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_with_cumulative_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3504d2bd-49ae-4d3e-a781-b806c506e11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_scenarios",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}